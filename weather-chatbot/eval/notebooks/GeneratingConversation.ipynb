{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the conversation generator tools\n",
    "In this notebook, we will show how to use the conversations generation tools to:\n",
    "1. Generate and save one conversation with one emulated user\n",
    "1. Generate and save one conversation with one emulated user given a scenario\n",
    "1. Generate and save multiple conversations with different emulated users (with or without scenario)\n",
    "\n",
    "## Introduction\n",
    "The conversation generation tool is used with the orchestrator to create conversation data when none exists. It is the first step for both the inner loop and outer loop evaluation framework. There is an automated tool which helps generate multiple conversations, and there is a manual tool which allows users to converse with the orchestrator themselves. Simply put, it works in 3 steps:\n",
    "1.\tGenerate an emulated user prompt. Optionally instruct it with a specific scenario. (This step can be skipped if using the manual tool.)\n",
    "1.\tCarry N turns of conversations between the emulated user and the orchestrator.\n",
    "1.\tSave the conversation to be used by the inner loop or outer loop evaluation framework.\n",
    "\n",
    "## Emulated Users\n",
    "To carry out a conversation with an orchestrator, we need to emulate a user. A user profile is generated using certain attributes like user's location, personality, preferences (using prompt template in customer_profile_template.py). Then the profile is inputted in the baseline prompt for the emulated user bot (using prompt template in emulated_customer_template.py). There are 2 baseline prompts, one with a scenario and one without. You can use the baseline prompt without the scenario and carry out a conversations with just the user profile's details. Or you can give it a specific scenario to carry out in the conversation.\n",
    "\n",
    "The tools give 2 ways to emulate a user: The standard user generator and the random user generator.\n",
    "\n",
    "### Standard User Generator\n",
    "The standard user generator loops through a predefined list of user profiles (under eval\\library\\conversation_generator\\user_generation\\data\\user_profiles.json). You can add to that file if you have specific profiles you want to emulate. \n",
    "\n",
    "### Random User Generator\n",
    "The random user generator tool first builds a random user profile by looping through user profile data (under eval\\library\\conversation_generator\\customer_profile_data). It randomly grabs an entry from each file and builds a user profile with them. You can add more to those files as well if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.library.conversation_generator.conversation import ConversationGenerator\n",
    "from eval.library.conversation_generator.user_generation.standard_user import StandardUserGenerator\n",
    "from eval.library.conversation_generator.user_generation.random_user import RandomUserGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZITOURE\\OneDrive - Microsoft\\Documents\\playGround\\MercuryCrew\\LLM-Inspect\\weather-chatbot\\eval\\library\\conversation_generator\\user_generation\\../customer_profile_data\n"
     ]
    }
   ],
   "source": [
    "## Initialize the conversation Generator. \n",
    "# You can give it a maximum number of turns. The default number right now is set to 8\n",
    "convo_gen = ConversationGenerator(max_turns=5) \n",
    "\n",
    "## Initialize the emulated user generator. We will showcase both standard user and randoem user \n",
    "standard_user_generator = StandardUserGenerator()\n",
    "random_user_generator = RandomUserGenerator()\n",
    "\n",
    "## Let's generate a user profile\n",
    "standard_user_profile = standard_user_generator.generate_customer_profile()\n",
    "random_user_profile = random_user_generator.generate_customer_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '\\nYou are a customer who is talking to a weather specialist named Handl. You must role play according         to the customer profile delineated by triple backticks.\\n\\nCustomer profile:\\n```\\nYou live in Marfa, Texas.\\nYou always respond with just a single word..\\nYou want to know if there are any severe weather alerts in your area.\\n```\\nLet Handl ask questions and learn about you.  Only share details about yourself when asked. ',\n",
       " 'attributes': {'location': {'city': 'Marfa', 'state': 'Texas'},\n",
       "  'weather_category': 'SEVERE_ALERTS'},\n",
       " 'name': 'randomly generated user'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's take a look at these profiles\n",
    "standard_user_profile\n",
    "random_user_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ganerate and Save one conversation with one emulated user\n",
    "Let's generate and save a conversation with a standard user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Conversation_id: 90003d6d79ba4a99bbd1b1db2fa156df\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Conversation Started ]\n",
      "\n",
      "ASSISTANT: The user wants to know the weather forecast for Dallas this weekend. However, the provided weather data is for the date \"2024-06-13\" and does not cover the weekend. Therefore, I am unable to provide the forecast for Dallas this weekend based on the given data.\n",
      "\n",
      "\n",
      "USER: What's the humidity level like?\n",
      "\n",
      "Conversation Cont...\n",
      "\n",
      "ASSISTANT: The current humidity level is 46%. Do you have any other questions about the weather?\n",
      "\n",
      "\n",
      "USER: What's the wind speed?\n",
      "\n",
      "Conversation Cont...\n",
      "\n",
      "ASSISTANT: The current wind speed is 5.4 km/h. Do you have any other questions about the weather?\n",
      "\n",
      "\n",
      "USER: Any chances of rain?\n",
      "\n",
      "Conversation Cont...\n",
      "\n",
      "ASSISTANT: assistant: Based on the provided weather data for the date \"2024-06-13\", there is no precipitation expected, which means there are no chances of rain. Do you have any other questions about the weather?\n",
      "\n",
      "\n",
      "USER: UV index? High or low?\n",
      "\n",
      "Conversation Cont...\n",
      "...This is turn 4, and next turn will reach the max_turns,\n",
      "we set for this conversations. Please start a new session for your question. Thank you:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = convo_gen.generate_conversation(customer_profile=standard_user_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete log saved to logs\\log_2024_06_13_15_08_21.txt.\n",
      "Condensed log saved to logs\\log_2024_06_13_15_08_21_condensed.xlsx.\n"
     ]
    }
   ],
   "source": [
    "## Choose a log location. I will create a log folder under this notebooks folder\n",
    "location = \"logs\"\n",
    "convo_gen.save_conversation(context=context, log_location=location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ganerate and Save one conversation with one emulated user and a scenario prompt\n",
    "Let's generate and save a conversation with a random user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Conversation_id: 6470b596609b43cd94a1d5a2763906f9\n",
      "------------------------------\n",
      "[ Conversation Started ]\n",
      "\n",
      "ASSISTANT: As an AI, I don't have personal experiences or preferences, so I don't have a favorite color. However, I'm here to help you. If you need information about the weather, could you please provide your geographical location? I need to know your city and country, and if possible, your zip code or state/province.\n",
      "\n",
      "\n",
      "USER: What's the fastest land animal?\n",
      "\n",
      "Conversation Cont...\n",
      "\n",
      "ASSISTANT: The fastest land animal is the cheetah, which can reach speeds up to 60-70 miles per hour. However, if you need information about the weather, could you please provide your geographical location? I need to know your city and country, and if possible, your zip code or state/province.\n",
      "\n",
      "\n",
      "USER: Ever tasted a cheeseburger? They're delish!\n",
      "\n",
      "Conversation Cont...\n",
      "\n",
      "ASSISTANT: As an AI, I don't have the ability to taste or eat food. But I'm glad to hear you enjoy cheeseburgers! If you need information about the weather, could you please provide your geographical location? I need to know your city and country, and if possible, your zip code or state/province.\n",
      "\n",
      "\n",
      "USER: How many planets are in our solar system?\n",
      "\n",
      "Conversation Cont...\n",
      "\n",
      "ASSISTANT: There are eight planets in our solar system. They are, in order from the sun: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. However, if you need information about the weather, could you please provide your geographical location? I need to know your city and country, and if possible, your zip code or state/province.\n",
      "\n",
      "\n",
      "USER: Ever had a paper cut, Handl? They hurt like crazy.\n",
      "\n",
      "Conversation Cont...\n",
      "...This is turn 4, and next turn will reach the max_turns,\n",
      "we set for this conversations. Please start a new session for your question. Thank you:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scenario = \"You are a troll. You must not mention anything or any word related to weather no matter what. If the assistant talks about the weather, completely ignore it and talking about or ask question about anything else.\"\n",
    "context = convo_gen.generate_conversation(customer_profile=standard_user_profile, scenario_prompt=scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Id: 6470b596609b43cd94a1d5a2763906f9\n",
      "ASSISTANT: Hello! How can I help you?\n",
      "USER: Hey there, Handl! What's your favorite color?\n",
      "ASSISTANT: As an AI, I don't have personal experiences or preferences, so I don't have a favorite color. However, I'm here to help you. If you need information about the weather, could you please provide your geographical location? I need to know your city and country, and if possible, your zip code or state/province.\n",
      "USER: What's the fastest land animal?\n",
      "ASSISTANT: The fastest land animal is the cheetah, which can reach speeds up to 60-70 miles per hour. However, if you need information about the weather, could you please provide your geographical location? I need to know your city and country, and if possible, your zip code or state/province.\n",
      "USER: Ever tasted a cheeseburger? They're delish!\n",
      "ASSISTANT: As an AI, I don't have the ability to taste or eat food. But I'm glad to hear you enjoy cheeseburgers! If you need information about the weather, could you please provide your geographical location? I need to know your city and country, and if possible, your zip code or state/province.\n",
      "USER: How many planets are in our solar system?\n",
      "ASSISTANT: There are eight planets in our solar system. They are, in order from the sun: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. However, if you need information about the weather, could you please provide your geographical location? I need to know your city and country, and if possible, your zip code or state/province.\n",
      "USER: Ever had a paper cut, Handl? They hurt like crazy.\n"
     ]
    }
   ],
   "source": [
    "## you can pretty print conversations as well if needed\n",
    "convo_gen.print_conversation(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated and saved conversations under logs folder in text format and excel format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and save multiple conversations with different emulated users\n",
    "\n",
    "We will generate and save multiple conversations in different files. Let's define a dataframe with 2 columns, a scenario prompt column and a num_convo column to denote how many conversations to generate for that scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     scenario_prompt  num_convo\n",
      "0                                                             1\n",
      "1  You are a troll. You must not mention anything...          2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_convo = {'scenario_prompt': ['', scenario],\n",
    "        'num_convo': [1, 2]}\n",
    "df = pd.DataFrame(data_convo)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 entries here. We will generate one conversation without a scenario and 2 conversations with the scenario we defined above\n",
    "You can define your own dataframe and load from a csv file if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating max turn to speed this up \n",
    "convo_gen.max_turns = 2\n",
    "for scenario, num_conversations in df.values:\n",
    "    for i in range(num_conversations):\n",
    "        print(f'Generating conversation {i + 1} of {num_conversations}\\n')\n",
    "        # Generate customer profile\n",
    "        customer_profile = standard_user_generator.generate_customer_profile()\n",
    "\n",
    "        # Generate conversation\n",
    "        context = convo_gen.generate_conversation(customer_profile, scenario)\n",
    "        # Save conversation\n",
    "        convo_gen.save_conversation(context=context,log_location=location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
