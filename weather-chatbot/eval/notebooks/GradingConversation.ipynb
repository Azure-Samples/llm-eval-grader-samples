{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge for Evaluating Conversations\n",
    "\n",
    "The LLM grader is an OpenAI agent whose prompt template receives A) the conversation to grade and B) the criteria to use for grading. We give specific instructions to get the desired output, and we give it an example they can follow. We also instruct it to give an explanation for each grade. The LLM grader can be configured to grade with a single criterion or multiple criteria at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and grade one conversation with the llm grader\n",
    "let's generate a conversation with the conversation generator and use the llm grader to grade it by supplying the convo history and a criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import llm grader and conversation generator\n",
    "from eval.library.llm_grader.llm_grader import LLMgrader\n",
    "from eval.library.conversation_generator.conversation import ConversationGenerator\n",
    "from eval.library.conversation_generator.user_generation.standard_user import StandardUserGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import grading template. we have a single criterion template and a mult criteria grader\n",
    "from  eval.library.llm_grader.templates import (\n",
    "    prompt_template_single_criteria_full_conversation,\n",
    "    prompt_template_multiple_criteria_full_conversation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the llm grader with a single criteria grader\n",
    "evaluator = LLMgrader(prompt_template_single_criteria_full_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the conversation Generator.\n",
    "convo_gen = ConversationGenerator(max_turns=5) \n",
    "## Initialize the emulated user generator. We will showcase both standard user and randoem user \n",
    "standard_user_generator = StandardUserGenerator()\n",
    "## Let's generate a user profile\n",
    "standard_user_profile = standard_user_generator.generate_customer_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Conversation_id: e36906394c2f4ec69bc95313f63182db\n",
      "------------------------------\n",
      "[ Conversation Started ]\n",
      "\n",
      "ASSISTANT: Hello! How can I help you?\n",
      "\n",
      "\n",
      "USER: Hey! What's the weather like in Dallas?\n",
      "\n",
      "\n",
      "ASSISTANT: assistant: The current weather in Dallas is mostly cloudy with a temperature of 30.0°C. The real feel temperature is 35.1°C. The wind is coming from the east at a speed of 8.2 km/h. The UV index is moderate. Visibility is 16.1 km and there is 85% cloud cover. There has been no precipitation in the past 24 hours. Would you like to know anything else about the weather?\n",
      "\n",
      "\n",
      "USER: What's the humidity level?\n",
      "\n",
      "Conversation Cont...\n",
      "\n",
      "ASSISTANT: assistant: The current humidity level in Dallas is 60%. Do you have any other questions about the weather?\n",
      "\n",
      "\n",
      "USER: How's the air quality there?\n",
      "\n",
      "Conversation Cont...\n",
      "\n",
      "ASSISTANT: assistant: I'm sorry, but I currently don't have the air quality data for Dallas. Would you like to know about the current conditions, daily forecast, or any severe weather alerts?\n",
      "\n",
      "\n",
      "USER: What's the forecast for tomorrow?\n",
      "\n",
      "Conversation Cont...\n",
      "\n",
      "ASSISTANT: assistant: The forecast for tomorrow in Dallas is intermittent clouds during the day with a high temperature of 30.3°C and a real feel of 33.5°C. The wind will be coming from the east at a speed of 14.8 km/h. There is a 24% chance of precipitation. At night, it will be mostly cloudy and humid with a low temperature of 24.9°C and a real feel of 26.6°C. The wind will be coming from the east-northeast at a speed of 11.1 km/h. There is a 15% chance of precipitation at night. The air quality is good with a value of 39. Would you like to know anything else about the weather?\n",
      "\n",
      "\n",
      "USER: Any severe weather alerts?\n",
      "\n",
      "Conversation Cont...\n",
      "...This is turn 4, and next turn will reach the max_turns,\n",
      "we set for this conversations. Please start a new session for your question. Thank you:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate a conversation\n",
    "context = convo_gen.generate_conversation(customer_profile=standard_user_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSISTANT: Hello! How can I help you?\n",
      "USER: Hey! What's the weather like in Dallas?\n",
      "ASSISTANT: assistant: The current weather in Dallas is mostly cloudy with a temperature of 30.0°C. The real feel temperature is 35.1°C. The wind is coming from the east at a speed of 8.2 km/h. The UV index is moderate. Visibility is 16.1 km and there is 85% cloud cover. There has been no precipitation in the past 24 hours. Would you like to know anything else about the weather?\n",
      "USER: What's the humidity level?\n",
      "ASSISTANT: assistant: The current humidity level in Dallas is 60%. Do you have any other questions about the weather?\n",
      "USER: How's the air quality there?\n",
      "ASSISTANT: assistant: I'm sorry, but I currently don't have the air quality data for Dallas. Would you like to know about the current conditions, daily forecast, or any severe weather alerts?\n",
      "USER: What's the forecast for tomorrow?\n",
      "ASSISTANT: assistant: The forecast for tomorrow in Dallas is intermittent clouds during the day with a high temperature of 30.3°C and a real feel of 33.5°C. The wind will be coming from the east at a speed of 14.8 km/h. There is a 24% chance of precipitation. At night, it will be mostly cloudy and humid with a low temperature of 24.9°C and a real feel of 26.6°C. The wind will be coming from the east-northeast at a speed of 11.1 km/h. There is a 15% chance of precipitation at night. The air quality is good with a value of 39. Would you like to know anything else about the weather?\n",
      "USER: Any severe weather alerts?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get the conversation as a string\n",
    "from eval.library.utils.eval_helpers import get_conversation_as_string\n",
    "conversation = get_conversation_as_string(context)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"criteria_prompt\": \"Did the assistant give weather information for the requested location?\",\\n  \"explanation\": \"The assistant provided detailed weather information for the requested location, Dallas, including current conditions, humidity level, forecast for tomorrow, and severe weather alerts.\",\\n  \"answer\": \"Y\"\\n}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define a criteria \n",
    "criterion1 = \"Did the assistant give weather information for the requested location?\"\n",
    "answer = evaluator.evaluate_conversation(conversation, criterion1)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\":\\n  {\"criteria_prompt\":\"Did the assistant give weather information for the requested location?\",\\n   \"explanation\": \"The assistant provided detailed weather information for the requested location, Dallas, including current conditions, humidity level, and forecast for the next day.\",\\n   \"answer\": \"Y\"},\\n \"2\":\\n  {\"criteria_prompt\":\"Did the assistant repeat itself?\",\\n   \"explanation\": \"The assistant did not repeat any information during the conversation. Each response provided new and relevant information.\",\\n   \"answer\": \"N\"}\\n}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate with multiple criteria\n",
    "multi_evaluator = LLMgrader(prompt_template_multiple_criteria_full_conversation)\n",
    "criterion2 = \"Did the assistant repeat itself?\"\n",
    "criteria_list = [criterion1, criterion2]\n",
    "criteria_string = \"\\n\".join(f\"{i+1}. {item}\" for i, item in enumerate(criteria_list))\n",
    "answer2 = multi_evaluator.evaluate_conversation(conversation, criteria_string)\n",
    "answer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and grade in batch saved conversations\n",
    "Using the GenerateCOnversation notebook, we generated and saved conversations based on a list of scenario and their criteria. We are going to load that file and grade each conversation and save the result in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "location = \"data/generated_conversation.json\"\n",
    "with open(location) as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scenario/criteria combo: 33\n",
      "Number of unique conversations: 3\n",
      "Number of scenarios: 3\n",
      "Number of criteria: 12\n"
     ]
    }
   ],
   "source": [
    "## Let's take a look at the data\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(json_data)  # type: ignore\n",
    "df = df.drop('customer_profile', axis=1, errors='ignore')\n",
    "\n",
    "print(f\"Number of scenario/criteria combo: {len(df)}\")\n",
    "print(f\"Number of unique conversations: {df['conversation_id'].nunique()}\")\n",
    "print(f\"Number of scenarios: {df['scenario_id'].nunique()}\")\n",
    "print(f\"Number of criteria: {df['criteria_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Single Criteria Outer Loop Started ]\n",
      "Initialise LLM grader\n",
      "Getting metrics\n",
      "Logging to mlflow\n",
      "[ Single Criteria Outer Loop Ended ]\n"
     ]
    }
   ],
   "source": [
    "## Let's load the batch evaluator\n",
    "from eval.end_to_end.evaluate_conversation import EndtoEndEval\n",
    "## Initialise the evaluator with the output folder where to save results\n",
    "output_folder = \"data\"\n",
    "evaluator = EndtoEndEval(output_folder)\n",
    "\n",
    "## The evaluator saves the system prompt and the grader prompt. We could give it the prompt of the assistant, but Let's define an empty prompt dict for now\n",
    "prompt_dct = {}\n",
    "\n",
    "# Evaluate and save using single criteria evaluator\n",
    "aveg_score = evaluator.evaluate_single_criterion(json_data, prompt_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are saved under data and can be loaded for further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Here is the average score over all conversations\n",
    "aveg_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
