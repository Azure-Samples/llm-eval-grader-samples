# Extending the Framework

The framework can be extended in several ways to support different use cases. Some of the ways to extend the framework are:

### Adding new Evaluation Metrics

The framework is currently having one evaluation metric, which is the `Turn Relevance Score`. This metrics is calculated by using Prompt Flow, the respective flow can be found in [azureml/promptflow/turn_relevance](../azureml/promptflow/turn_relevance) folder.

To add a new evaluation metric, you can follow the below steps:

1. Create a new Prompt Flow for the evaluation metric and add to the `promptflow` folder.
1. Modify the [evaluation_config.yml](../azureml/pipeline/config/evaluation_config.yml) to include the new evaluation metric.
    1. Add a new evaluator under `evaluators` section. You can refer to the existing evaluator for the structure.
    1. Add the new evaluator to the respective app in the `apps.evaluators` section. You can refer to the existing app for the structure. Make sure to give a unique name to the `endpoint_name`.
    1. The evaluator can be turned on/off by setting the `active` flag to `true` or `false`.
1. Deploy the evaluation pipeline using the [deployment script](../azureml/pipeline/deploy/deploy_evaluation_pipeline.py). For more details refer to the [Framework Deployment](./02_Deployment.md) guide.
1. Execute the evaluation pipeline using the [run_evaluation_pipeline.py](../azureml/pipeline/run/run_evaluation_pipeline.py). For more details refer to the [Framework Deployment](./02_Deployment.md) guide.

### Adding new Visualization

The framework uses PowerBI for visualizing the evaluation metrics. The PowerBI dashboard is available in the [dashboards](../dashboards) folder. You can add the new visualization to the PowerBI dashboard by following the below steps:

1. Open the PowerBI file in the PowerBI Desktop and refresh the data. For more details refer to the [Demo](./04_Demo.md) guide step 6 and 7.
1. Add the new card to the PowerBI dashboard that visualizes the new evaluation metric. The data will be available in following tables:
    1. `dim_metric` - Contains the evaluation metrics name and ID, this can be used to filter data for a specific metric in a card.
    1. `dim_conversation` - Contains the conversation data, we can use either `conv_start_time` or `conv_end_time` for the time data in X-axis. Or for time range filter.
    1. `fact_evaluation_metric` - Contains the evaluation metrics data, we can use the `metric_str_value` or `metric_numeric_value` based on the type of metric as the Y-axis data. This fact table is linked to `dim_metric` and `dim_conversation` tables via `metric_id` and `conversation_id` respectively. You can see this relationship in the PowerBI data model.

### Adding new ChatBot

So far we have talked about how we can add new evaluation metrics and the visualization for the evaluation metrics. But what if you want to add a new ChatBot to the framework? The framework is designed to be extensible, so you can add a new ChatBot by following the below steps:

1. Make sure the ChatBot is deployed and storing the logs in the Azure Application Insights. The logs should be in the format that can be processed by the framework. The following logs should be stored:
    1. For Conversation Data (For each query the final response that user gets from the chatbot):
        1. Message will be `conversation_data`
        1. We need to add the following fields in the `customDimensions`:
            1. `conversation_id` - A unique ID for the conversation.
            1. `turn_id` - A unique ID for the turn in the conversation.
            1. `query` - The query entered by the user.
            1. `response` - The response that user gets from the chatbot.
    1. For LLM Data (For each OpenAI GPT model call):
        1. Message will be `llm_data`
        1. We need to add the following fields in the `customDimensions`:
            1. `conversation_id` - A unique ID for the conversation.
            1. `turn_id` - A unique ID for the turn in the conversation.
            1. `query` - The query entered by the user.
            1. `llm_response` - The response generated by the OpenAI GPT model. This should be as is returned by the model.
    1. The logs should be stored in the `traces` table in the Application Insights.
    1. You can additional metadata in the `customDimensions` as per your requirement. Ex. `intent`, `model`, etc. That will go to `dim_metadata` table in the PowerBI data model.
    1. For the reference you can refer to the [sample chatbot](../sample-chatbot) that is part of the framework.
1. Modify the [transformation_config.yml](../azureml/pipeline/config/transformation_config.yml) file and add the new ChatBot to the `transformation_config` section. You can refer to the existing chatbot for the structure. There are few important fields that you need to pay attention:
    1. `workspace_id_secret_key` - The secret key for the Azure Application Insights workspace, that is added as a secret in the Azure Key Vault during the infrastructure deployment by the [Infrastructure Deployment](./01_Infrastructure.md) guide.
    1. For each type of logs the `mappings` section handles the columns mapping from the logs to the data model. You can refer to the existing chatbot for the structure.
    1. `endpoint_name` should be unique for each chatbot.
1. Deploy the transformation pipeline using the [deployment script](../azureml/pipeline/deploy/deploy_transformation_pipeline.py). For more details refer to the [Framework Deployment](./02_Deployment.md) guide.
1. Deploy the evaluation pipeline using the [deployment script](../azureml/pipeline/deploy/deploy_evaluation_pipeline.py). For more details refer to the [Framework Deployment](./02_Deployment.md) guide.
1. Execute the transformation pipeline using the [run_transformation_pipeline.py](../azureml/pipeline/run/run_transformation_pipeline.py). For more details refer to the [Framework Deployment](./02_Deployment.md) guide.
1. Execute the evaluation pipeline using the [run_evaluation_pipeline.py](../azureml/pipeline/run/run_evaluation_pipeline.py). For more details refer to the [Framework Deployment](./02_Deployment.md) guide.
1. Add the new visualization to the PowerBI dashboard by following the steps mentioned in the [Adding new Visualization](#adding-new-visualization) section.

### Extending for a specific use case

This framework is designed to be modular and extensible, so you can extend the framework for a specific use case.

For example you can modify the transformation pipeline in the following ways:

1. Modify the [sampling](../src/llminspect/transformation/sampling.py) logic to sample the data in a different way.
1. Modify the [transformation](../src/llminspect/transformation/transform.py) logic to transform the data in a different way.
1. Modify the [goldzone_prep](../src/llminspect/transformation/goldzone_prep.py) to prepare the data for the evaluation according to updated data model. For example adding new metadata to the data model, or adding a new dimension to the data model, etc.
1. Modify the [transform_data.py](../azureml/pipeline/components/code/transform_data.py) pipeline entry script to include the new transformation code block into the orchestration. Also if there are any new parameters that are required for the transformation, you can add them to the `transformation_config.yml` and pass them to the transformation pipeline.

Also you can modify the evaluation pipeline in the following ways:

1. Modify the [prep_data.py](../azureml/pipeline/components/code/prep_data.py) to prepare the data according to the new evaluation metric. For example while evaluating entire conversation, you can add a new logic to group all the turns in a conversation and evaluate the conversation as a whole.
1. Modify the [write_metrics.py](../azureml/pipeline/components/code/write_metrics.py) to write the evaluation metrics according to the new evaluation metric or metrics store data model. For example if the evaluation metric is a calculated score, you can add a new logic to calculate the score based on the evaluation metric.

In summary, this framework contains the building blocks that can be used to extend the framework for a specific use case. You can modify the existing components or add new components to the framework to support the specific use case.
